from torch import nn, optim
import torch
import torch.nn.functional as F
import pytorch_lightning as pl
import torchmetrics
# from src.losses import WeightedMSELoss

class convnet_sc_regressor(nn.Module):
    """ 
    Single stream conv net to ingest full-disk magnetograms based on Subhamoy Chatterjee's architecture

    Parameters:
        dim (int):    square dimension of input image
        length (int): number of images in a sequence
        dropoutRatio (float):   percentage of disconnections for Dropout

    """
    def __init__(self, dim:int=256, length:int=1, len_features:int=0, weights=[], dropoutRatio:float=0.0):
        super().__init__()
        self.len_features = len_features

        self.block1 = nn.Sequential(
            nn.Conv2d(length, 32, (3,3),padding='same'),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d((2,2), stride=(2,2))
        )
        self.block2 = nn.Sequential(
            nn.Conv2d(32, 32, (3,3),padding='same'),
            nn.BatchNorm2d(32),
            nn.ReLU(inplace=True),
            nn.MaxPool2d((2,2), stride=(2,2))
        )
        self.block3 = nn.Sequential(
            nn.Conv2d(32, 64, (3,3),padding='same'),
            nn.BatchNorm2d(64),
            nn.ReLU(inplace=True),
            nn.MaxPool2d((2,2), stride=(2,2))
        )
        self.block4 = nn.Sequential(
            # nn.ZeroPad2d((2,2)),
            nn.Conv2d(64, 128, (3,3),padding='same'),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d((2,2), stride=(2,2))
        )
        self.block5 = nn.Sequential(
            nn.Conv2d(128, 256, (3,3),padding='same'),
            nn.ReLU(inplace=True),
        )

        if len_features == 0:
            self.fcl = nn.Sequential(
                nn.LazyLinear(100),
                nn.ReLU(inplace=True),
                nn.Dropout1d(dropoutRatio),
                nn.Linear(100,1),
                nn.ReLU(inplace=True),
            )
        else:
            self.fcl = nn.Sequential(
                nn.LazyLinear(100),
                nn.ReLU(inplace=True),
                nn.Dropout1d(dropoutRatio),
            )
        
        self.fcl2 = nn.Sequential(
            nn.Linear(100+len_features,100),
            nn.ReLU(inplace=True),
            nn.Dropout1d(dropoutRatio),
            nn.Linear(100,1),   
        )
        
        self.forward(torch.ones(1,length,dim,dim),torch.ones(1,len_features))
        self.apply(self._init_weights)

        # coeff intercept for LR model on totus flux [[6.18252855]][-3.07028227]
        # coeff intercept for LR model on all features [[ 1.35617824  0.5010206  -0.56691345  1.85041399  0.7660414   0.55303976 2.42641335  1.67886773  1.88992678  2.84953033]] [-3.85753394]
        if len(weights)!=0:
            with torch.no_grad():
                # self.fcl2[0].weight[0,-len(weights)+1:] = torch.Tensor(weights[1:])
                # self.fcl2[0].bias[0] = weights[0]
                self.fcl2[0].weight[:,-len_features:] = torch.Tensor(weights[0]).transpose(0,1)
                self.fcl2[0].bias[:] = torch.Tensor(weights[1])
                self.fcl2[3].weight[0,:] = torch.Tensor(weights[2]).view(len(weights[2]))
                self.fcl2[3].bias[0] = torch.Tensor(weights[3])
                
    def _init_weights(self,module):
        """
            Function to check for layer instances within the model and initialize
            weights and biases.   We are using glorot/xavier uniform for the 2D convolution
            weights and random normal for the linear layers.  All biases are initilized
            as zeros.
        """
        if isinstance(module,nn.Conv2d):
            nn.init.xavier_uniform_(module.weight)
            module.bias.data.zero_()

        if isinstance(module,nn.Linear):
            nn.init.xavier_normal_(module.weight)
            module.bias.data.zero_()

        if isinstance(module,nn.LazyLinear):
            nn.init.xavier_normal_(module.weight)
            module.bias.data.zero_()


    def forward(self,x,f):
        
        x = self.block1(x)
        x = self.block2(x)
        x = self.block3(x)
        x = self.block4(x)
        x = self.block5(x)
        x = x.view(x.shape[0],-1)
        x = self.fcl(x)

        # # append features
        if self.len_features > 0:
            x = torch.cat([x,f],dim=1)
            x = self.fcl2(x)
        return x
    
class convnet_mini_regressor(nn.Module):
    """ 
    Single stream conv net with 2 convolutional layers and 2 fully connected layers to ingest assembled embeddings

    Parameters:
        dim (int):    square dimension of input image
        length (int): number of images in a sequence
        dropoutRatio (float):   percentage of disconnections for Dropout

    """
    def __init__(self, dim:int=16, length:int=16, len_features:int=0, weights=[], dropoutRatio:float=0.0):
        super().__init__()
        self.block1 = nn.Sequential(
            nn.Conv2d(length, 128, (2,2),padding=0),
            nn.BatchNorm2d(128),
            nn.ReLU(inplace=True),
            nn.MaxPool2d((2,2), stride=(2,2))
        )
        self.block2 = nn.Sequential(
            nn.Conv2d(128, 256, (2,2),padding='same'),
            nn.ReLU(inplace=True),
        )

        self.fcl = nn.Sequential(
            nn.LazyLinear(100),
            nn.ReLU(inplace=True),
            nn.Dropout1d(dropoutRatio),
            nn.Linear(100,1),
        )
        
        self.fcl2 = nn.Sequential(
            nn.Linear(1+len_features,1),
            nn.Sigmoid()
        )
        
        self.forward(torch.ones(1,length,dim,dim),torch.ones(1,len_features))
        self.apply(self._init_weights)

        # coeff intercept for LR model on totus flux [[6.18252855]][-3.07028227]
        # coeff intercept for LR model on all features [[ 1.35617824  0.5010206  -0.56691345  1.85041399  0.7660414   0.55303976 2.42641335  1.67886773  1.88992678  2.84953033]] [-3.85753394]
        if len(weights)!=0:
            with torch.no_grad():
                self.fcl2[0].weight[0,1:] = torch.Tensor(weights[1:])
                self.fcl2[0].bias[0] = weights[0]

    def _init_weights(self,module):
        """
            Function to check for layer instances within the model and initialize
            weights and biases.   We are using glorot/xavier uniform for the 2D convolution
            weights and random normal for the linear layers.  All biases are initilized
            as zeros.
        """
        if isinstance(module,nn.Conv2d):
            # nn.init.zeros_(module.weight)
            nn.init.xavier_uniform_(module.weight)
            module.bias.data.zero_()

        if isinstance(module,nn.Linear):
            # nn.init.zeros_(module.weight)
            nn.init.xavier_normal_(module.weight)
            module.bias.data.zero_()

        if isinstance(module,nn.LazyLinear):
            # nn.init.zeros_(module.weight)
            nn.init.xavier_normal_(module.weight)
            module.bias.data.zero_()


    def forward(self,x,f):
        
        x = self.block1(x)
        x = self.block2(x)
        x = x.view(x.shape[0],-1)
        x = self.fcl(x)

        # append features
        # x = torch.cat([x,f],dim=1)
        # x = self.fcl2(x)
        return x

class LitConvNetRegressor(pl.LightningModule):
    """
        PyTorch Lightning module to classify magnetograms as flaring or non-flaring

        Parameters:
            model (torch.nn.Module):    a PyTorch model that ingests magnetograms and outputs a binary classification
            lr (float):                 learning rate
            wd (float):                 L2 regularization parameter
            epochs (int):               Number of epochs for scheduler
    """
    def __init__(self,model,lr:float=1e-4,wd:float=1e-2,epochs:int=100):
        super().__init__()
        self.model = model

        # Save original values before wandb modifies them 
        self._lr_value = lr
        self._wd_value = wd
        self._epochs_value = epochs

        #original 
        self.lr = lr
        self.weight_decay = wd
        self.epochs = epochs

        # define loss function
        # self.loss = WeightedMSELoss()   
        self.loss = nn.MSELoss()

        # define metrics fo training, validation and test phases
        self.train_mse = torchmetrics.MeanSquaredError()
        self.train_mae = torchmetrics.MeanAbsoluteError()
        self.train_r2 = torchmetrics.R2Score()

        self.val_mse = torchmetrics.MeanSquaredError()
        self.val_mae = torchmetrics.MeanAbsoluteError()
        self.val_r2 = torchmetrics.R2Score()

        self.test_mse = torchmetrics.MeanSquaredError()

        self.test_confusion_matrix = torchmetrics.ConfusionMatrix(task='binary',num_classes=2)

    def training_step(self,batch,batch_idx):
        """
            Expands a batch into image and label, runs the model forward and 
            calculates loss.

            Parameters:
                batch:                  batch from a DataLoader
                batch_idx:              index of batch                  

            Returns:
                loss (torch tensor):    loss evaluated on batch
        """
        fname, x, f, y = batch
        y = y.view(y.shape[0],-1)
        y_hat = self.model(x,f)
        loss = self.loss(y_hat,y.type_as(y_hat))

        # calculate metrics
        self.train_mse(y_hat*6-8.5,y*6-8.5)
        self.train_mae(y_hat*6-8.5,y*6-8.5)
        self.train_r2(y_hat,y)

        self.log_dict({'loss':loss,
                       'train_mse':self.train_mse,
                       'train_mae':self.train_mae,
                       'train_r2':self.train_r2},
                       on_step=False,on_epoch=True)
        return loss
    
    def validation_step(self,batch,batch_idx):
        """
            Runs the model on the validation set and logs validation loss 
            and other metrics.

            Parameters:
                batch:                  batch from a DataLoader
                batch_idx:              index of batch                  
        """
        fname, x, f, y = batch
        y = y.view(y.shape[0],-1)
        # forward pass
        y_hat = self.model(x,f)
        val_loss = self.loss(y_hat,y.type_as(y_hat))

        # calculate metrics
        self.val_mse(y_hat*6-8.5,y*6-8.5)
        self.val_mae(y_hat*6-8.5,y*6-8.5)
        self.val_r2(y_hat,y)

        self.log_dict({
                      'val_loss':val_loss,
                      'val_mse':self.val_mse,
                      'val_mae':self.val_mae,
                      'val_r2':self.val_r2},
                      on_step=False,on_epoch=True)


    def test_step(self,batch,batch_idx):
        """
            Runs the model on the test set and logs test metrics 

            Parameters:
                batch:                  batch from a DataLoader
                batch_idx:              index of batch                  
        """
        fname, x, f, y = batch
        y = y.view(y.shape[0])
        # forward pass
        y_hat = self.model(x,f)
        y_hat = y_hat.squeeze(-1) 

        # calculate metrics
        self.test_mse(y_hat,y)
        self.test_confusion_matrix.update(y_hat,y)

        self.log_dict({'test_mse':self.test_mse},
                       on_step=False,on_epoch=True)

    def configure_optimizers(self):
        """
            Sets up the optimizer and learning rate scheduler.
            
            Returns:
                optimizer:              A torch optimizer
        """
        optimizer = optim.Adam(self.model.parameters(), lr=self._lr_value, weight_decay=self._wd_value)
        scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=self._epochs_value)
        return [optimizer],[scheduler]

    def predict_step(self,batch,batch_idx,dataloader_idx=0):
        """
            Forward pass of model for prediction

            Parameters:
                batch:          batch from a DataLoader
                batch_idx:      batch index
                dataloader_idx

            Returns:
                fname (tensor):  file names for samples
                y_true (tensor): true labels for the batch
                y_pred (tensor): model outputs for the batch
        """
        fname, x, f, y = batch
        y = y.view(y.shape[0],-1)
        return fname, y, self.model(x,f)
    
    def on_load_checkpoint(self, checkpoint: dict) -> None:
        state_dict = checkpoint["state_dict"]
        model_state_dict = self.state_dict()
        is_changed = False
        for k in state_dict:
            if k in model_state_dict:
                if state_dict[k].shape != model_state_dict[k].shape:
                    state_dict[k] = model_state_dict[k]
                    is_changed = True
            else:
                is_changed = True

        if is_changed:
            checkpoint.pop("optimizer_states", None)
